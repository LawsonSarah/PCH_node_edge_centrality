{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b483c99-563a-4726-939c-a8076687a7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DELETE OUTPUTS AND RESTART KERNAL BEFORE RUNNING\n",
    "#RUN THIS FILE TO REPRODUCE RESULTS FROM \n",
    "#\"An application of node and edge nonlinear hypergrpah centrality to a protein complex hypernetwork.\" Lawson, S., Donovan, D., Lefevre,J. School of Mathematics and Physics, University of Queensland 2024 (unpublished). \n",
    "using JLD2 \n",
    "using SparseArrays\n",
    "using StatsBase\n",
    "using PyPlot\n",
    "using Plots\n",
    "using SimpleHypergraphs\n",
    "using Trapz\n",
    "using Graphs\n",
    "using LinearAlgebra\n",
    "using EvalMetrics\n",
    "using Combinatorics\n",
    "using DelimitedFiles\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\sarit\\\\RA_code\\\\NSVC_git_repo\\\\PCH_node_edge_centrality\" #file path for code folder\n",
    "output_path = \"$file_path\\\\Outputs\" #output folder for text files and plots\n",
    "include(\"$file_path\\\\data_processing_functions.jl\")\n",
    "include(\"$file_path\\\\mappings_initialization.jl\") \n",
    "include(\"$file_path\\\\compute_centrality_functions.jl\")\n",
    "include(\"$file_path\\\\analysis_functions.jl\") \n",
    "include(\"$file_path\\\\helper_functions.jl\") \n",
    "include(\"$file_path\\\\plot_functions.jl\") \n",
    "include(\"$file_path\\\\high_performance.jl\") \n",
    "lbbc_cent_file=\"$file_path\\\\yeast_raw_data\\\\LBBC_centralities.txt\"\n",
    "\n",
    "#CHOOSE FUNCTION SETS - COMMENT OUT ALL FUNCTION SETS OTHER THAN ONE NEEDED  \n",
    "varying = \n",
    "#\"setA\"\n",
    "#\"setB_summary\"\n",
    "\"A_B_summary\" #Also used for LBBC comparison \n",
    "                       \n",
    "\n",
    "# \"1_1_c_k/c\"   #Full set of c, d=k/c for c,k in range 0.1:95  presaved as setA_1_1_c_kc_centralities-load below\n",
    "# \"setB_complete\" #   #Full set B centralitites for a,b,c,d presaved as setB_comb_0_1_195_95_centralities-load below\n",
    "\n",
    "#Algorithm parameters\n",
    "max_iterations = 800\n",
    "tolerance = 1e-6\n",
    "\n",
    "#percentage of essential proteins as edge essentiality criteria:\n",
    "\n",
    "e_percentage = 0.6  #at least 60% essential proteins in complex\n",
    "\n",
    "\n",
    "#6 percentage thresholds \n",
    "percent_threshold = [0.05, 0.10, 0.25, 0.50, 0.75, 1.0];\n",
    "\n",
    "\n",
    "#Process yeast complexes and essentiality data:\n",
    "dataset_name = \"yeast_protein\"\n",
    "name_title = \"Yeast Protein Complexes\"\n",
    "file = \"$file_path\\\\yeast_raw_data\\\\raw_complex_yeast.txt\"\n",
    "essential = \"$file_path\\\\yeast_raw_data\\\\essential_proteins_yeast.txt\"\n",
    "nonessential = \"$file_path\\\\yeast_raw_data\\\\nonessential_proteins_yeast.txt\"\n",
    "set_dict = yeast_complexes_set_dict(file) #create dictionary of complexes from raw file\n",
    "\n",
    "#Remove single member sets from set_dict:\n",
    "s = 0\n",
    "for i in keys(set_dict)\n",
    "    if length(set_dict[i]) == 1\n",
    "        delete!(set_dict, i)\n",
    "        s += 1\n",
    "    end\n",
    "end\n",
    "println(s, \" single member complexes removed\")\n",
    "\n",
    "#create text files to outputs folder and a dict with key:node name and value: node number. \n",
    "node_numbers_dict = data_processing(set_dict, dataset_name, output_path)\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "#CREATE HYPERGRAPH\n",
    "B, w, edgesx, edges_fromB, node_names_list, connected = read_hypergraph_data(\n",
    "    dataset_name,\n",
    "    output_path,\n",
    "    max_iterations = max_iterations,\n",
    "    tolerance = tolerance,\n",
    ");\n",
    "\n",
    "if !connected\n",
    "    #recaluclate for largest connected component:\n",
    "    #save original data:\n",
    "    orig_w = w\n",
    "    orig_B = B\n",
    "    orig_edges_fromB = edges_fromB\n",
    "    orig_set_dict = set_dict\n",
    "    orig_node_numbers_dict = node_numbers_dict\n",
    "    orig_node_names_list = node_names_list\n",
    "    orig_edges = edgesx\n",
    "    println(\"recalculating on largest connected component.\")\n",
    "    B, w, edgesx, edges_fromB, node_names_list, set_dict, dataset_name, node_numbers_dict =\n",
    "        max_component_recalculate(B, node_numbers_dict, set_dict, dataset_name)\n",
    "\n",
    "end\n",
    "\n",
    "#count nodes, edges and original sets\n",
    "no_nodes = size(B)[1]\n",
    "no_edges = size(B)[2]\n",
    "no_sets = Int(sum(w))\n",
    "\n",
    "println(\n",
    "    \"Hypergraph has \n",
    "$no_edges edges containing $no_sets set(s).\",\n",
    ")\n",
    "\n",
    "#dictionary containing key:node, value:list of edges node belongs to \n",
    "node_edges_dict = node_in_edges(node_numbers_dict, edges_fromB)\n",
    "\n",
    "#dictionary containing key:edge number value:original sets represented\n",
    "edge_number_dict = edge_numbers_dict(node_numbers_dict)\n",
    "\n",
    "#create dictionary of node and edge degrees\n",
    "ia = false #if ia = false then does not include max_edge, node adjacency, mean_deg in degree_centralities\n",
    "deg_centralities, edge_deg_centralities =\n",
    "    deg_centralities_dict(B, node_edges_dict, edges_fromB, edge_weights = w, inc_adj = ia)\n",
    "\n",
    "#create dictionary of nodes and edge essentiality status and print number of essential/nonessential nodes/edges\n",
    "node_ess_dict, unk, n_essent, n_non_essent =\n",
    "    node_class_dict(node_names_list, essential, nonessential, printlines = false)\n",
    "\n",
    "edge_ess_dict, e_essent, e_non_essent = edge_class_dict(\n",
    "    node_ess_dict,\n",
    "    edges_fromB,\n",
    "    node_names_list,\n",
    "    printlines = false,\n",
    "    per_ess = e_percentage,\n",
    ");\n",
    "\n",
    "println(\"Nodes essential: \", n_essent, \" Edges essential: \", e_essent)\n",
    "\n",
    "#################################################################################################################\n",
    "#Create or load centrality dictionary. \n",
    "\n",
    "#Comment out following if uploading saved dictionaries\n",
    "centralities, ranked_nodes_dict, ranked_edges_dict, mappings =\n",
    "    initialization(varying); #centralitites based on 'varying' parameter\n",
    "\n",
    "#large centrality dictionaries have been saved and can be preloaded:\n",
    "#centralities=load_object(\"setA_1_1_c_kc_centralities.jld2\") #centralitites for functions of the form P:(1,1,c,k/c) for c,k in range 0.1:95\n",
    "#centralities=load_object(\"setB_comb_0_1_195_95_centralities.jld2\") #centralitites for functions of the form P:(a,b,c,d) with a,c in {0,1,95,1/95} and b,d in {1,95,1/95}}\n",
    "\n",
    "#print summary of maximum true positives for centralities\n",
    "println(\"________________________________________________\")\n",
    "for cent_type in [\"x\", \"y\"]\n",
    "    for i in percent_threshold\n",
    "        max_true_pos(\n",
    "            cent_type,\n",
    "            centralities,\n",
    "            i,\n",
    "            n_essent,\n",
    "            e_essent,\n",
    "            n_non_essent,\n",
    "            e_non_essent,\n",
    "            node_ess_dict,\n",
    "            edge_ess_dict;\n",
    "            printlines = false,\n",
    "            reverse = true,\n",
    "        )\n",
    "    end\n",
    "\n",
    "    println(\"________________________________________________\")\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "#comment out the following if centrality dictionary is large e.g. usio\\ng large uploaded dictionaries\n",
    "# print summary of AUC and create plots\n",
    "\n",
    "for cent_type in [\"x\", \"y\"]\n",
    "    measures_dict = all_class_measures_plots(\n",
    "        cent_type,\n",
    "        centralities,\n",
    "        deg_centralities,\n",
    "        edge_deg_centralities,\n",
    "        n_essent,\n",
    "        n_non_essent,\n",
    "        node_ess_dict,\n",
    "        e_essent,\n",
    "        e_non_essent,\n",
    "        edge_ess_dict,\n",
    "        percent_threshold,\n",
    "        printlines = false,\n",
    "        varying = varying\n",
    "    )\n",
    "end\n",
    "\n",
    "##print out max auc for node and edge high performers for setA_1_1_c_kc_centralities.jld2 and setB_comb_0_1_195_95_centralities.jld2\n",
    "\n",
    "#high_peform(n_essent, n_non_essent, e_essent, e_non_essent, node_ess_dict,edge_ess_dict)\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "#analyse intersection of MIPS-PIN and PCH\n",
    "\n",
    "println(\"\\n----------------PIN_PCH INTERSECTION ANALYSIS-----------------------\")\n",
    "#create cent_dictionary from LBBC-MIPS text file:\n",
    "lbbc_dict=read_LBBC_cent(lbbc_cent_file)\n",
    "lbbc_all_names=collect(keys(lbbc_dict))\n",
    "\n",
    "#create essentiality status dictionary for all nodes in MIPS-PIN\n",
    "lbbc_node_ess_dict,u,e,ne=node_class_dict(lbbc_all_names, essential, nonessential; printlines=true)\n",
    "\n",
    "\n",
    "#list of centralities corresponding to name at same index in lbbc_all_names\n",
    "lbbc_all_cent=[]\n",
    "for key in lbbc_all_names\n",
    "    push!(lbbc_all_cent,lbbc_dict[key])\n",
    "end\n",
    "\n",
    "#list of essentiality corresponding to name at same index in lbbc_all_names\n",
    "lbbc_all_status=[]\n",
    "for name in lbbc_all_names\n",
    "        push!(lbbc_all_status,lbbc_node_ess_dict[name] )\n",
    "end\n",
    "\n",
    "#create vectors for ranked by name, essentiality and centrality \n",
    "index = sortperm(vec(lbbc_all_cent),rev=true) #indexes for order of centralities\n",
    "lbbc_ranked_cent=lbbc_all_cent[index] #rank by cent\n",
    "lbbc_ranked_names=lbbc_all_names[index] #rank by name\n",
    "lbbc_ranked_status=lbbc_all_status[index] #rank by essentiality\n",
    "\n",
    "#rank essentiality by binary\n",
    "lbbc_ranked_binary=[] #excluding unknowns\n",
    "for i in lbbc_ranked_status\n",
    "    if i==\"essential\"  \n",
    "        push!(lbbc_ranked_binary,1)\n",
    "    elseif i==\"nonessential\"\n",
    "        push!(lbbc_ranked_binary,0)\n",
    "    end\n",
    "end\n",
    "\n",
    "#Restrict to intersection\n",
    "\n",
    "#list of proteins in intersection by name\n",
    "int_prots= readlines(\"$file_path\\\\yeast_raw_data\\\\PIN_PCH_intersection.txt\")\n",
    "\n",
    "#create LBBC dictionary key:node_number to value:centrality (matching to proteins node nunmber in PCH):\n",
    "lbbc_cent=Dict()\n",
    "for key in keys(lbbc_dict) #for each protein in MIPS\n",
    "    if key in keys(node_numbers_dict) #if protein in NSVC PCH\n",
    "        lbbc_cent[node_numbers_dict[key]]=lbbc_dict[key] #add key:protein node number value:lbbc centrality\n",
    "    end\n",
    "end\n",
    "\n",
    "#sorted array of node numbers for list of intersecting proteins\n",
    "sorted_int_nodes=sort([node_numbers_dict[i] for i in int_prots])\n",
    "\n",
    "#list of lbbc centralities ordered by node number\n",
    "lbbc_cents=[lbbc_cent[i] for i in sorted_int_nodes]\n",
    "\n",
    "#create intersection centralities dictionary and add LBBC centralities\n",
    "int_centralities=Dict(\"LBBC\" => Dict(\"x\" => lbbc_cents, \"y\"=> [0]))\n",
    "\n",
    "function_sets=[\"P:(0,1,0,1)\", \"P:(1/95,95,1,1/95)\",\"P:(1,1,46,29/46)\"]\n",
    "#ensure these centralities have been calculated in current centralities dictionary (ie for the chosen 'varying' parameter)\n",
    "\n",
    "#reduce centrality values for specified function sets in active NSVC centralities dict to nodes in intersection and add to intersect dictionary\n",
    "for key in keys(centralities)\n",
    "    if key in function_sets\n",
    "        red_cents_1=[centralities[key][\"x\"][i] for i in sorted_int_nodes]\n",
    "        int_centralities[key]= Dict(\"x\" => red_cents_1, \"y\"=> [0])\n",
    "    end\n",
    "end \n",
    "\n",
    "#node_ess_dict for intersect nodes:\n",
    "int_node_ess_dict=Dict()\n",
    "for i in int_prots\n",
    "        int_node_ess_dict[i]=node_ess_dict[i]\n",
    "end\n",
    "\n",
    "#list of essential and non-essential nodes\n",
    "int_ness_nodes=[name for name in int_prots if node_ess_dict[name]==\"nonessential\" ] \n",
    "int_ess_nodes=[name for name in int_prots if node_ess_dict[name]==\"essential\" ] \n",
    "int_n_essent=length(int_ess_nodes)\n",
    "int_n_non_essent=length(int_ness_nodes)\n",
    "\n",
    "println(\"number of nodes in intersection: \", length(int_prots))\n",
    "println(\"essential nodes in intersection: \", int_n_essent )\n",
    "println(\"nonessential nodes in intersection: \", int_n_non_essent)\n",
    "\n",
    "\n",
    "#perform analysis\n",
    "cent_type=\"x\"\n",
    "measures_dict = all_class_measures_plots(\n",
    "    cent_type,\n",
    "    int_centralities,\n",
    "    deg_centralities,\n",
    "    edge_deg_centralities,\n",
    "    int_n_essent,\n",
    "    int_n_non_essent,\n",
    "    int_node_ess_dict,\n",
    "    e_essent,\n",
    "    e_non_essent,\n",
    "    edge_ess_dict,\n",
    "    percent_threshold,\n",
    "    printlines = false,\n",
    "    varying = varying,\n",
    "    pin_pch_int=true, \n",
    "    int_node_numbers=sorted_int_nodes\n",
    ")\n",
    "\n",
    "println(\"PLOTS \\n\n",
    "    PCH node: 1)Prec-rec 2)Box 3)Cumulative; \\n\n",
    "    PCH edge: 4)Prec-rec 5)Box 6)Cumulative; \\n\n",
    "    PIN-PCH Intersection: node 7)Prec-rec, 8)Box 9)Cumulative \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
